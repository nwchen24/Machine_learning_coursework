{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Models: the Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GMM.html\n",
    "\n",
    "In this notebook, I have a few tasks for you to try out related to GMMs. Not much guidance is given here, but the things to do are pretty straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.qda import QDA\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering \n",
    "from sklearn.cluster import KMeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data generation\n",
    "np.random.seed(1)\n",
    "\n",
    "covar = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
    "\n",
    "dat_threedee = np.vstack((np.random.multivariate_normal([1, 1, 1], covar, 250), \n",
    "                          np.random.multivariate_normal([1, 5, 1], covar, 250),\n",
    "                          np.random.multivariate_normal([3, 2, 1], covar, 250)))\n",
    "\n",
    "dat_twodee = np.vstack((np.random.multivariate_normal([0, 0], [[10, 0], [0, 1]], 100),\n",
    "                        np.random.multivariate_normal([0, 5], [[1, 0.5], [0.5, 1]], 100),\n",
    "                        np.random.multivariate_normal([5, 5], [[5, 3], [3, 2]], 100)))\n",
    "\n",
    "dat_twodee_simple = np.vstack((np.random.multivariate_normal([0, 0], [[1, 0], [0, 1]], 100),\n",
    "                               np.random.multivariate_normal([0, 5], [[1, 0], [0, 1]], 100),\n",
    "                               np.random.multivariate_normal([5, 5], [[2, 0], [0, 2]], 100)))\n",
    "\n",
    "dat_twodee_headache = np.vstack((np.random.multivariate_normal([0, 0], [[1, 0], [0, 1]], 100),\n",
    "                                 np.random.multivariate_normal([0, 1], [[1, 0], [0, 1]], 100),\n",
    "                                 np.random.multivariate_normal([1, 1], [[2, 0], [0, 2]], 100)))\n",
    "\n",
    "comps = 2\n",
    "gm_mod = GaussianMixture(n_components = comps)\n",
    "\n",
    "gm_mod.fit(dat_twodee)\n",
    "y_hat = gm_mod.predict(dat_twodee)\n",
    "\n",
    "print \"The mean(s) estimated with this\", comps, \"component GMM: \\n\", gm_mod.means_\n",
    "\n",
    "print \"\\n\\nThe covariances [SEE THE DOCS] estimated with this\", comps, \"component GMM: \\n\", gm_mod.covariances_ \n",
    "\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "p = plt.subplot(1, 1, 1)\n",
    "p.scatter(dat_twodee[:, 0], dat_twodee[:, 1], c=y_hat, cmap=cm_bright)\n",
    "plt.title(\"Mysterious Rabbit Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common questions:\n",
    "1. The two means of the estimated gaussian distributions are printed out, add them as points to the plot. Do their locations make sense to you?\n",
    "2. The two covariance matrices are not the full matrices (since it is 2-D gaussian, this should be a 2x2 matrix). What are the full matrices? Construct them (they are not available directly as output from the GMM fit) and print them out. You can see what the method is giving by looking at the documentation for 'covars' in the GMM docs, look at the 'spherical' case, which is the default we are using here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Covariance Structures, components, AIC / BIC\n",
    "\n",
    "The goal of this question is to try out the flexibility of the class of GMM models, and to try out methods for choosing a model.\n",
    "\n",
    "Use the data:\n",
    "\n",
    "dat_twodee\n",
    "\n",
    "1. Try out the different covariance structures, see the argument `covariance_type`  Plot the results.\n",
    "\n",
    "2. Try out different numbers of components, see the argument `n_components` Plot the results.\n",
    "\n",
    "3. Use BIC to find the best choice for the settings in (1) and (2), see the `BIC` function, an example appears below. What model did you find works best? What model would you expect to work best, given how we generated the data?\n",
    "\n",
    "4. Try all of the above with dat_twodee_headache; what results do you see here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# demo of the BIC function\n",
    "print \"Demo of BIC, a lower score relative to another model is better:\"\n",
    "print \"BIC =\", gm_mod.bic(dat_twodee)\n",
    "\n",
    "# demo for colors for up to 6 components so you can plot multiple component models\n",
    "cm_bright = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00FF', '#00FFFF', '#FFFF00'])\n",
    "\n",
    "p = plt.subplot(1, 1, 1)\n",
    "p.scatter(range(6), range(6), c=range(6), cmap=cm_bright)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM vs Kmeans vs Agglomerative Clustering\n",
    "\n",
    "The goal of this question is to directly compare GMMs to other clustering techniques, and get an intuitive sense of the differences.\n",
    "\n",
    "Links to docs for kmeans and agglomerative clustering:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering\n",
    "\n",
    "Use the data:\n",
    "\n",
    "dat_twodee\n",
    "\n",
    "1. Compare the results you see with GaussianMixture, KMeans, and AgglomerativeClustering -- use all the three linkage types. Some demos are provided below. Try different numbers of clusters for all these algorithms. What differences do you see between the methods? Which one would you recommend using? Don't worry about trying to numerically score the models here, just use plots and your visual judgement.\n",
    "\n",
    "2. Try this again with the data: dat_twodee_simple and dat_twodee_headache; do you see any differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Solution code goes here!\n",
    "\n",
    "cm_bright = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00FF', '#00FFFF', '#FFFF00'])\n",
    "\n",
    "def Proc():\n",
    "    # define this function to take in the number of clusters\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    \n",
    "    p = plt.subplot(1, 5, 1)\n",
    "    p.scatter(range(6), range(6), c=range(6), cmap=cm_bright)\n",
    "    plt.title('KMeans')\n",
    "    \n",
    "    p = plt.subplot(1, 5, 2)\n",
    "    p.scatter(range(6), range(6), c=range(6), cmap=cm_bright)\n",
    "    plt.title('GaussianMixture') \n",
    "    \n",
    "    p = plt.subplot(1, 5, 3)\n",
    "    p.scatter(range(6), range(6), c=range(6), cmap=cm_bright)\n",
    "    plt.title('Agglomerative') \n",
    "    \n",
    "    p = plt.subplot(1, 5, 4)\n",
    "    p.scatter(range(6), range(6), c=range(6), cmap=cm_bright)\n",
    "    plt.title('Agglomerative') \n",
    "    \n",
    "    p = plt.subplot(1, 5, 5)\n",
    "    p.scatter(range(6), range(6), c=range(6), cmap=cm_bright)\n",
    "    plt.title('Agglomerative') \n",
    "\n",
    "Proc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating and fitting data for GMMs\n",
    "\n",
    "The goal of this question is to see how GMMs do at recovering parameters. That is, if the data really came from a certain GMM, when and how well can we figure that out?\n",
    "\n",
    "Build off of the data generation code given below (particularly for 1) and the data generation code in the common section at the top.\n",
    "\n",
    "1. Make a function that generates spherical data for an arbitrary number of components and dimensions. This should have three arguments: a number of draws per component, a single scalar variance for all components and a matrix of means for the components. Try fitting a GMM with the appropriate number of components: can you recover the means? Try increasing the variance / separation of the cluster centers, can you recover the means?\n",
    "\n",
    "2. Generate data that is more appropriate for other `covariance_type` settings: focus on diag and full. **Do this in only two dimensions, more dimensions will be a bit of a pain to visualize**. Plot the generated data, run GMMs and see how well you can recover the _mean_ and _variance_ of each component. You only need to use 2 or 3 components here.\n",
    "\n",
    "NOTE: in all of the above, you may fit GMMs pretending you know the number of components (not really like real life...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# some example code for generating data...\n",
    "draws_per_comp = 5\n",
    "variance = 1\n",
    "means = np.array([[0, 0], [10, 10], [10, -10]])\n",
    "\n",
    "GMM()\n",
    "\n",
    "print means.shape\n",
    "\n",
    "covar = np.diag([variance for i in range(means.shape[1])])\n",
    "\n",
    "gen_dat = np.vstack([np.random.multivariate_normal(means[i, :], covar, draws_per_comp) for i in range(means.shape[0])])\n",
    "\n",
    "gm_mod = GaussianMixture(n_components=3)\n",
    "\n",
    "gm_mod.fit(gen_dat)\n",
    "\n",
    "cm_bright = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "    \n",
    "p = plt.subplot(1, 3, 1)\n",
    "p.scatter(gen_dat[:, 0], gen_dat[:, 1], c=gm_mod.predict(gen_dat), cmap=cm_bright)\n",
    "plt.title('GMM generated data') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
