{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This notebook is intended to introduce you to running ipython notebook and to familiarize you with some basics of numpy, matplotlib, and sklearn, which you'll use extensively in this course. Read through the commands, try making changes, and make sure you understand how the plots below are generated.\n",
    "\n",
    "In your projects, you should focus on making your code as readable as possible. Use lots of comments -- see the code below -- and try to prefer clarity over compact code.\n",
    "\n",
    "You should also familiarize yourself with the various keyboard shortcuts for moving between cells and running cells. Ctrl-ENTER runs a cell, while shift-ENTER runs a cell and advances focus to the next cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first code cell just contains setup calls -- importing libraries and some other global settings to make things run smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import a bunch of libraries.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate evenly spaced X values in [0,1] using linspace.\n",
    "X is a numpy array, in particular a multi-dimensional \"ndarray\".\n",
    "Try looking at the documentation for ndarray:\n",
    "http://docs.scipy.org/doc/numpy/reference/arrays.ndarray.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many samples to generate. Try adjusting this value.\n",
    "n_samples = 20\n",
    "\n",
    "# Evenly spaced\n",
    "X = np.linspace(0, 1, n_samples)\n",
    "\n",
    "# Inspect X.\n",
    "print X\n",
    "print type(X)\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a \"true\" function that we will try to approximate with a model, below. We'll use python's lambda syntax, which makes it easy to define a simple function in a single line. See here for more details:\n",
    "\n",
    "http://www.python-course.eu/lambda.php\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the true function as a piece of a cosine curve.\n",
    "true_function = lambda x: np.cos(1.5 * np.pi * x)\n",
    "\n",
    "# Try it out. Notice that you can apply the function to a scalar or an array.\n",
    "print true_function(0)\n",
    "print true_function(0.5)\n",
    "print true_function(np.array([0, 0.5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generate noisy observations of our true function. This simulates something like the situation we encounter in the real world: we observe noisy data from which we'd like to infer a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate true y values.\n",
    "y = true_function(X)\n",
    "\n",
    "# Print the values of y to the nearest hundredth.\n",
    "print ['%.2f' %i for i in y]\n",
    "\n",
    "# Add random noise to y.\n",
    "# The randn function samples random numbers from the standard Normal distribution.\n",
    "# Multiplying adjusts the standard deviation of the distribution.\n",
    "y += np.random.randn(n_samples) * 0.2\n",
    "\n",
    "# Print the noise-added values of y for comparison.\n",
    "print ['%.2f' %i for i in y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Now we have some outputs, y, that we want to predict, and some inputs X. In general, our outputs (in this course) will always be 1-dimensional. Our inputs will usually have more than 1 dimension -- we'll call these our features. But here, for simplicity, we have just a single feature. \n",
    "\n",
    "Since the machine learning classes in sklearn expect input feature vectors, we need to turn each input x in X into a feature vector [x]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Another way to do this is np.transpose([X]). Read more about array indexing for details.\n",
    "X = X[:, np.newaxis]\n",
    "print X\n",
    "print X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "We're going to use lots of samples like this. Let's write a function to automate the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_sample_data(nsamp, true_function):\n",
    "    \"\"\"Noisy samples of true_function on the interval (0,1)\n",
    "    \n",
    "    Args: \n",
    "        nsamp: number of evenly spaced samples on (0,1)\n",
    "        true_function: function to apply to each sample\n",
    "        \n",
    "    Returns:\n",
    "        X: evenly space points on (0,1) as a column vector\n",
    "        actual: the value of true_function at each point in X\n",
    "        y: function values with noise added\n",
    "    \"\"\"\n",
    "    \n",
    "    # Insert your code here\n",
    "\n",
    "    # return X, actual, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate_sample_data(10, true_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you're already familiar with linear regression, let's try that first. Check out the sklearn documentation for linear regression:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try setting fit_intercept=False as well.\n",
    "lr = LinearRegression(fit_intercept=True)\n",
    "lr.fit(X, y)\n",
    "print lr.intercept_\n",
    "print lr.coef_\n",
    "print 'Estimated function: y = %.2f + %.2fx' %(lr.intercept_, lr.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what is happening graphically. Here is a function to compare actual and fitted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_fitted_function(X, y, actual, model):\n",
    "    \"\"\"Plot actual vs fitted values of model\"\"\"\n",
    "    plt.scatter(X, y, label = 'Samples')\n",
    "    plt.plot(X, actual, label = 'True Function')\n",
    "    plt.plot(X, model.predict(X), label = 'Model')\n",
    "    plt.legend(loc='best', fontsize='x-small')\n",
    "    plt.setp(plt.gca(), xticks=(), yticks=())\n",
    "    plt.xlim(-.05, 1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some data, fit a linear regression model, and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, actual, y = generate_sample_data(20, true_function)\n",
    "lr = LinearRegression(fit_intercept=True)\n",
    "lr.fit(X, y)\n",
    "compare_fitted_function(X, y, actual, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear model doesn't work too well for fitting this data - it just isn't complex enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Repeat the process above for five _different_ samples of 20 points. Place the plots side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nsamp = 20\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "for i in range(5):\n",
    "    ax = plt.subplot(1, 5, i+1)\n",
    "    \n",
    "    # Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model has ___high bias___\n",
    "- The model is always wrong, even when you try different training data. In fact, it's always wrong _in the same way_.\n",
    "- The model does not have enough complexity to match the underlying function.\n",
    "- This is an example of underfitting\n",
    "\n",
    "On the other hand, the model exhibits ___low variance___.\n",
    "- The learned model is basically the same, no matter which (random) sample is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximating a cosine function with a linear model doesn't work so well. By adding polynomial transformations of our feature(s), we can fit more complex functions. This is often called polynomial regression. Take a look at the sklearn documentation for the PolynomialFeatures preprocessor:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n",
    "\n",
    "You'll notice that the sklearn classes have many of the same function names like fit() and fit_transform()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Try increasing the degree past 2.\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X2 = poly.fit_transform(X)\n",
    "print X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit a linear model where the input features are (x, x^2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression(fit_intercept=True)\n",
    "lr.fit(X2, y)\n",
    "print lr.intercept_\n",
    "print lr.coef_\n",
    "print 'Estimated function: y = %.2f + %.2fx0 + %.2fx1' %(lr.intercept_, lr.coef_[0], lr.coef_[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use sklearn's Pipeline framework to connect the polynomial expansion and the linear regression into one 'model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "degree = 2\n",
    "poly = PolynomialFeatures(degree = degree, include_bias=False)\n",
    "lr = LinearRegression(fit_intercept=True)\n",
    "pipeline = Pipeline([(\"polynomial_features\", poly),\n",
    "                     (\"linear_regression\", lr)])\n",
    "pipeline.fit(X, y)\n",
    "compare_fitted_function(X, y, true_function(X), pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the best degree of polynomial to use? What happens when the degree is too high?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Generate 5 model comparison plots. For each plot,\n",
    "- Generate 20 evenly spaced (noisy) samples\n",
    "- Fit a 10 degree polynomial to the sample.\n",
    "- Plot the fitted vs actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The degree 10 polynomial model has ___high variance___\n",
    "- Slightly different training sets result in very different models.\n",
    "- This is an example of overfitting\n",
    "- Models with high variance do not generalize well. That is, since training data is different from testing data, a model that is optimal for the training data could be very different from the model that would have been optimal for the test data.\n",
    "\n",
    "On the other hand, the model exhibits ___low bias___.\n",
    "- A 10 degree polynomial is plenty complex to model the underlying function.\n",
    "- In fact, if you were to carry out the above process 1000s of times, you would see that _on average_ the model predictions match the actual values. (Try it!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- What happens to a high bias model when you add more training data? What about a high variance model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "Machine learning is a constant battle with the _bias-variance tradeoff_. Simple models tend to be biased, but as we add complexity to reduce the bias, we also increase the variance---which can lead to overfitting.\n",
    "\n",
    "Let's explore this concept to find the optimal degree of polynomial to fit 20 samples from a cosine curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Write a function that returns training and testing error\n",
    "\n",
    "Hint: Try looking at the predict method in the LinearRegression object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_error(X, y, actual, model):\n",
    "    \"\"\"Get training and testing error\n",
    "    \n",
    "    Input:\n",
    "        X: evenly spaced points on (0,1)\n",
    "        y: noisy observations of the true function\n",
    "        actual: true value of the function\n",
    "        model: linear or polynomial regression model\n",
    "        \n",
    "    Returns:\n",
    "        train_error: average squared error on the training data\n",
    "        test_error: average squared error, compared to the true function\n",
    "    \"\"\"\n",
    "    # Your code goes here\n",
    "    \n",
    "    # return train_error, test_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_error(X, y, actual, pipe)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Plot degree of polynomial vs train/test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_errs = []\n",
    "test_errs = []\n",
    "X, actual, y = generate_sample_data(20, true_function)\n",
    "degrees = range(1, 13)\n",
    "\n",
    "for deg in degrees:\n",
    "    \n",
    "    # Fit a polynomial model\n",
    "    # Your code goes here\n",
    "    \n",
    "    # Append the training and testing error\n",
    "    # your code goes here\n",
    "    \n",
    "\n",
    "# Plot\n",
    "plt.plot(degrees, train_errs, label='Training Error')\n",
    "plt.plot(degrees, test_errs, label='True Error')\n",
    "plt.legend()\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Which degree of polynomial would you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 3rd degree polynomial is best. For some runs, the 4th degree polynomial has a slightly lower true error, but the gain is marginal. A simpler model is always preferred, when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
