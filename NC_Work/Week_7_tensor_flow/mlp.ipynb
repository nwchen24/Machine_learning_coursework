{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Tensorflow\n",
    "\n",
    "### Goals\n",
    "- Gain a basic understanding of the what/how/why of Tensorflow\n",
    "- Implement a simple multi-layer perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow (and other 'deep learning' libraries) are really good at gradient descent. \n",
    "\n",
    "Three types of objects\n",
    "- Placeholders where we will use real data\n",
    "- Variables. These are the model parameters - they can be updated using gradient descent.\n",
    "- Constants.\n",
    "\n",
    "Use these objects to construct a loss function. Then use gradient descent to find the best parameters, given the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "node1 = tf.constant(3.0, tf.float32)\n",
    "node2 = tf.constant(4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.0, 3.0, 6.0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run([node1 + node2, node1, 2*node1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders\n",
    "Placeholders are the objects that will be filled with real data at runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "\n",
    "adder_node = a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 11.  ,   0.19], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(adder_node, feed_dict = {a:[3, .19], b:[8,0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about the linear equation\n",
    "$$\n",
    "y = 3 x - 3\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = tf.Variable([3.], tf.float32)\n",
    "b = tf.Variable([-3.], tf.float32)\n",
    "\n",
    "linear_model = W * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables need to be initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  3.,  6.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(linear_model, feed_dict = {x:[1, 2, 3]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we could define some y values and see how well it fits the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.,  1.,  1.], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "error = tf.square(linear_model - y)\n",
    "\n",
    "\n",
    "sess.run(error, {x:[1, 2, 3], y:[3, 4, 5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crime Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load some crime data\n",
    "headers = pd.read_csv('comm_names.txt', squeeze=True)\n",
    "headers = headers.apply(lambda s: s.split()[1])\n",
    "crime = (pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/communities/communities.data', \n",
    "                    header=None, na_values=['?'], names=headers)\n",
    "         .iloc[:, 5:]\n",
    "         .dropna()\n",
    "         )\n",
    "\n",
    "# Set target and predictors\n",
    "target = 'ViolentCrimesPerPop'\n",
    "predictors = [c for c in crime.columns if not c == target]\n",
    "\n",
    "# Train/test split\n",
    "X = crime[predictors]\n",
    "y = crime[[target]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We are going to estimate a linear regression model. First, we need to define all of the pieces of the model\n",
    "\n",
    "#parameters\n",
    "dim_input = X_train.shape[1]\n",
    "dim_output = 1\n",
    "\n",
    "#input\n",
    "#Array to match the shape of X train\n",
    "x = tf.placeholder(tf.float32, [None, dim_input])\n",
    "\n",
    "#output\n",
    "#column vector to match y_train\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "#variables\n",
    "#W is a matrix of model parameters, the dimensions of which will match the input - parameter for each input.\n",
    "#Individual X is a row, want to multiply the X's by the rows W\n",
    "#b is the beginning initialization of the intercepts\n",
    "#we initialize these randomly, then through the training step below, we will update the parameters to minimize the loss function\n",
    "W = tf.Variable(tf.random_normal([dim_input, dim_output]))\n",
    "b = tf.Variable(tf.random_normal([dim_output]))\n",
    "\n",
    "#Specify the functional form of the model\n",
    "y = tf.matmul(x, W) + b\n",
    "\n",
    "#Loss - define mean squared error\n",
    "mse = tf.reduce_mean(tf.square(y - y_))\n",
    "#define the regularization term which penalizes large parameters. Note the 11.8 here is arbitrary and can be played with\n",
    "reg = tf.reduce_mean(11.8 * tf.square(W))\n",
    "\n",
    "#define new loss function, we'll now minimize MSE plus regularization term\n",
    "loss = mse + reg\n",
    "\n",
    "#Optimizer\n",
    "#AdamOptimizer is an 'adaptive' gradient descent in that it adjusts step sizes as it goes\n",
    "optimizer = tf.train.AdamOptimizer(0.01)\n",
    "\n",
    "#minimize loss\n",
    "train_step = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121, 1)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view W's shape\n",
    "#W has 121 rows (it's a column vector), these are the coefficients for the 121 variables in the input\n",
    "#Above, we initialized these to random values. Gradient descent will find the true parameters\n",
    "sess.run(W).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.780749"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(mse, {x:X_train, y_:y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run a training step (uses the optimizer above and tries to minimize mean squared error)\n",
    "#After this step, mse should decrease and ultimately converge\n",
    "sess.run(train_step, {x:X_train, y_:y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.6049089"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view error on training data to watch it converge\n",
    "sess.run(mse, {x:X_train, y_:y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.8574004"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#view error on testing data\n",
    "sess.run(mse, {x:X_test, y_:y_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1: Run 10000 gradient descent steps of the model above. Every 500 iterations, note the train error and the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 500 Train error is: 0.0267927, Test error is: 0.0265614\n",
      "\n",
      "Iteration 1000 Train error is: 0.0261429, Test error is: 0.0262311\n",
      "\n",
      "Iteration 1500 Train error is: 0.0261454, Test error is: 0.02628\n",
      "\n",
      "Iteration 2000 Train error is: 0.0261454, Test error is: 0.0263059\n",
      "\n",
      "Iteration 2500 Train error is: 0.0261457, Test error is: 0.0263278\n",
      "\n",
      "Iteration 3000 Train error is: 0.0261461, Test error is: 0.0263434\n",
      "\n",
      "Iteration 3500 Train error is: 0.0261463, Test error is: 0.0263525\n",
      "\n",
      "Iteration 4000 Train error is: 0.0261464, Test error is: 0.0263566\n",
      "\n",
      "Iteration 4500 Train error is: 0.0261465, Test error is: 0.026358\n",
      "\n",
      "Iteration 5000 Train error is: 0.0261465, Test error is: 0.0263583\n",
      "\n",
      "Iteration 5500 Train error is: 0.0261464, Test error is: 0.0263419\n",
      "\n",
      "Iteration 6000 Train error is: 0.0285499, Test error is: 0.027846\n",
      "\n",
      "Iteration 6500 Train error is: 0.0261465, Test error is: 0.0263584\n",
      "\n",
      "Iteration 7000 Train error is: 0.0261465, Test error is: 0.0263584\n",
      "\n",
      "Iteration 7500 Train error is: 0.0261465, Test error is: 0.0263584\n",
      "\n",
      "Iteration 8000 Train error is: 0.0261465, Test error is: 0.0263584\n",
      "\n",
      "Iteration 8500 Train error is: 0.0261463, Test error is: 0.0263545\n",
      "\n",
      "Iteration 9000 Train error is: 0.0261465, Test error is: 0.0263584\n",
      "\n",
      "Iteration 9500 Train error is: 0.0261465, Test error is: 0.0263589\n",
      "\n",
      "Iteration 10000 Train error is: 0.0261465, Test error is: 0.0263584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "\n",
    "train_errors = []\n",
    "\n",
    "while counter < 10000:\n",
    "    #run train step\n",
    "    sess.run(train_step, {x:X_train, y_:y_train})\n",
    "    #increment counter\n",
    "    counter += 1\n",
    "    #get errors\n",
    "    train_error = sess.run(mse, {x:X_train, y_:y_train})\n",
    "    test_error = sess.run(mse, {x:X_test, y_:y_test})\n",
    "    \n",
    "    train_errors.append(train_error)\n",
    "    if counter % 500 == 0:\n",
    "\n",
    "        print 'Iteration ' + str(counter) + ' Train error is: ' + str(train_error) + ', Test error is: ' + str(test_error)\n",
    "        print \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the regularized model converges much more quickly on the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhUAAAFkCAYAAACXcsmHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3XuUXWV9//H3dyYICjJEqKFUQCSZlIolzCCC/gQrgWCs\nt58u9ASD1lrv1d901Sqtba3WS601XhBb2wrU6LG069eqPwOjARusNl5mAKkiMxlRbK1BEhi5KUnm\n+f2xT5gzh7klnH32M5n3a62zOGfvnfN89zPDnM/Zz7P3jpQSkiRJD1dX1QVIkqQDg6FCkiS1haFC\nkiS1haFCkiS1haFCkiS1haFCkiS1haFCkiS1haFCkiS1haFCkiS1haFCkiS1RamhIiK6IuKdEfH9\niLgvIrZFxNvKbFOSJFVjScnv/1bg1cBFwHeB04DLI+KulNIlJbctSZI6qOxQcSbw2ZTS1Y3Xt0XE\nOuD0ktuVJEkdVvaciq8B50TECoCIOAV4GrCp5HYlSVKHlX2k4r3A4cD3ImIPRYj5o5TSZ6bbOCKO\nBNYAPwB+XnJtkiQdSA4BHg8MppR2VFFA2aHixcA64CUUcypWAR+KiB+nlD45zfZrgE+VXJMkSQey\nC4FPV9Fw2aHifcB7Ukr/1Hj9nYh4PHAxMF2o+AHAxo0bOemkk0ouLW8DAwNs2LCh6jKyYF8U7IdJ\n9kXBfphkX8DNN9/MS1/6Umh8llah7FDxKGBPy7IJZp7L8XOAk046ib6+vjLryl5PT8+i74O97IuC\n/TDJvijYD5Psiykqmz5Qdqj4PPC2iPgv4DtAHzAA/F3J7UqSpA4rO1S8AXgn8FHgscCPgY81lkmS\npANIqaEipXQv8HuNhyRJOoB5749M1Wq1qkvIhn1RsB8m2RcF+2GSfZGHSClVXcODIqIPGBoaGnLC\njSRJ+2B4eJj+/n6A/pTScBU1eKRCkiS1haFCkiS1haFCkiS1haFCkiS1haFCkiS1haFCkiS1haFC\nkiS1haFCkiS1haFCkiS1haFCkiS1haFCkiS1haFCkiS1haFCkiS1haFCkiS1haFCkiS1haFCkiS1\nhaFCkiS1haFCkiS1haFCkiS1haFCkiS1haFCkiS1RamhIiJujYiJaR4fKbNdSZLUeUtKfv/TgO6m\n108CvghcWXK7kiSpw0oNFSmlHc2vI+I5wFhK6StltitJkjqvY3MqIuIg4ELg7zvVpiRJ6pxOTtR8\nAdADXNHBNiVJUod0MlS8ArgqpfSTDrYpSZI6pOyJmgBExHHAauD589l+YGCAnp6eKctqtRq1Wq2E\n6vIzMjLC2NgYy5cvZ8WKFVWXI0nKTL1ep16vT1k2Pj5eUTWTIqVUfiMRbwd+Bzg2pTQxy3Z9wNDQ\n0BB9fX2l15WbnTt3sm7degYHNz24bM2atdTrG1m6dGmFlUmScjc8PEx/fz9Af0ppuIoaSh/+iIgA\nXg5cPlugEKxbt57Nm7cCG4HbgI1s3ryVWu2lFVcmSdLcOjH8sRo4FrisA20tWCMjI40jFBspTpIB\nuJA9exKDg+sZHR11KESSlLXSj1SklL6UUupOKW0ru62FbGxsrPHsrJY1ZwOwbZvdJ0nKm/f+yMSJ\nJ57YeHZdy5otACxfvryj9UiStK8MFZno7e1lzZq1dHe/kWII5EfARrq738SaNWsd+pAkZc9QkZF6\nfSOrV58BrAeOA9azevUZ1OsbK65MkqS5deQ6FZqfpUuXcvXVX2B0dJRt27Z5nQpJ0oJiqMjQihUr\nDBOSpAXH4Q9JktQWhgpJktQWhgpJktQWhgpJktQWhgpJktQWhgpJktQWhgpJktQWhgpJktQWhgpJ\nktQWhgpJktQWhgpJktQWhgpJktQWhgpJktQWhgpJktQWhgpJktQWhgpJktQWhgpJktQWhgpJktQW\nhgpJktQWpYeKiDgmIj4ZEXdExH0RcWNE9JXdriRJ6qwlZb55RBwBfBW4BlgD3AGsAO4ss11JktR5\npYYK4K3AbSmlVzYt+2HJbUqSpAqUPfzxHOBbEXFlRGyPiOGIeOWc/0qSJC04ZYeKJwCvBW4BzgM+\nBnw4ItaX3K4kSeqwsoc/uoBvpJT+uPH6xog4GXgN8MmZ/tHAwAA9PT1TltVqNWq1WmmFSpK0UNTr\nder1+pRl4+PjFVUzKVJK5b15xA+AL6aUXtW07DXAH6WUjp1m+z5gaGhoiL4+TxCRJGm+hoeH6e/v\nB+hPKQ1XUUPZwx9fBVa2LFuJkzUlSTrglB0qNgBnRMTFEXFiRKwDXglcUnK7kiSpw0oNFSmlbwEv\nAGrATcAfAW9KKX2mzHYlSVLnlT1Rk5TSJmBT2e1IkqRqee8PSZLUFoYKSZLUFoYKSZLUFqXPqdD+\nGRkZYWxsjOXLl7NixYqqy5EkaU4eqcjMzp07Of/8Z7Ny5UrWrl1Lb28v55//bO680xu7SpLyZqjI\nzLp169m8eSuwEbgN2MjmzVup1V5acWWSJM3O4Y+MjIyMMDi4iSJQXNhYeiF79iQGB9czOjrqUIgk\nKVseqcjI2NhY49lZLWvOBmDbtm0drUeSpH1hqMjIiSee2Hh2XcuaLQAsX768o/VIkrQvDBUZ6e3t\nZc2atXR3v5FiCORHwEa6u9/EmjVrHfqQJGXNUJGZen0jq1efAawHjgPWs3r1GdTrGyuuTJKk2TlR\nMzNLly7l6qu/wOjoKNu2bfM6FZKkBcNQkakVK1YYJiRJC4rDH5IkqS0MFZIkqS0MFZIkqS0MFZIk\nqS0MFZIkqS0MFZIkqS0MFZIkqS0MFZIkqS0MFZIkqS0MFZIkqS1KDRUR8acRMdHy+G6ZbUqSpGp0\n4t4f/wmcA0Tj9e4OtClJkjqsE6Fid0rppx1oR5IkVagTcypWRMR/R8RYRGyMiGM70KYkSeqwskPF\nVuDlwBrgNcAJwHURcWjJ7UqSpA4rdfgjpTTY9PI/I+IbwA+BC4DLymxbkiR1VifmVDwopTQeESPA\n8tm2GxgYoKenZ8qyWq1GrVYrszxJkhaEer1OvV6fsmx8fLyiaiZFSqlzjUUcBtwG/ElK6ZJp1vcB\nQ0NDQ/T19XWsrhyNjIwwNjbG8uXLWbFiRdXlSJIyNzw8TH9/P0B/Smm4ihrKvk7FX0bEWRFxfEQ8\nFfgXYBdQn+OfLlo7d+7k/POfzcqVK1m7di29vb2cf/6zufPOO6suTZKkWZU9UfNxwKeB7wGfAX4K\nnJFS2lFyuwvW85//QgYHvwpspDios5HBwa/y/Oe/sOLKJEmaXdkTNZ0EsQ9GRkb4ylf+jSJQPJni\numGnA5dw3XXrGR0ddShEkpQt7/2RkSuvvLLx7BPASmAt0MveE2Um10uSlB9DRUZuv/12ih/J9TQP\nfxSvuxrrJUnKU0dPKdXsTjnlFGAC+AhwYWPphUAC1nPqqadWVZokSXPySEVGjjnmmMazs1rWnA3A\nsmXLOlqPJEn7wiMVGenq2pvxrqOYqDlGcZ2wrwOwZIk/LklSvvyUysjExATFHeJ/G/hF05qDgWD3\nbu8aL0nKl8MfGTnxxBMpQsUhTJ2oeQgQLF8+69XNJUmqlEcqsjMBfJTpJmpKkpQzj1RkZGxsrPFs\n+oma27Zt62g9kiTtC0NFRorhDygmajbbAuDwhyQpaw5/ZKS3t5cjj1zGjh2vpxjyOJsiULyBI49c\n5iW6JUlZ80hFRkZGRtixYztwAsUciuMa/z2BHTu2Mzo6Wml9kiTNxlCRkck5FY9pWVO8dk6FJCln\nhoqMFHMqZr73h3MqJEk5c05Fdma+94ckSTnzSEVGtmzZ0ng2/Smlk+slScqPoSJL059SKklSzhz+\nyMhxxx1HkfN+l6mnlL4R6OL444+vsDpJkmZnqMhIcUOxCeBUps6heCZwrTcUkyRlzeGPjEze+nxn\ny5ritbc+lyTlzFCRkeJIRTdwK1NPKb0V6PZIhSQpa371zUhxpGIPM92l1CMVkqSceaQiI8WRCpjp\nlFKPVEiScmaoyIh3KZUkLWQdCxUR8daImIiID3SqzYWmt7eXNWvW0t39Roq5FD8CNtLd/SbWrFnr\nXUolSVnrSKiIiCcDrwJu7ER7C1m9vpHVq8+g+S6lq1efQb2+seLKJEmaXekz/yLiMIqv3a8E/rjs\n9ha6pUuXcvXVX+CLX/wiW7du5cwzz+Tcc8+tuixJkubUidMJPgp8PqV0bUQYKuawc+dO1q1bz+Dg\npgeXrVmzlnp9I0uXLq2wMkmSZlfq8EdEvARYBVxcZjsHknXr1rN581aar1OxefNWarWXVlyZJEmz\nK+1IRUQ8DvggsDqltGtf/u3AwAA9PT1TltVqNWq1WhsrzM/IyEjjCMVGmq9TsWdPYnBwPaOjo07W\nlCRRr9ep1+tTlo2Pj1dUzaRIKZXzxhHPA/4vxdWcorG4m+JKTnuAg1NL4xHRBwwNDQ3R19dXSl05\nu+qqq1i7di3FEYpjm9b8CDiOTZs28axnPaua4iRJWRseHqa/vx+gP6U0XEUNZQ5/bAaeRDH8cUrj\n8S2Kr+GntAYKeZ0KSdLCVtrwR0rpXuC7zcsi4l5gR0rp5rLaXcj2Xqdi8+Y3smfPj4FlwO10d7+H\n1au9ToUkKW+dvpmERyfmcOmlH+H005/Kjh1/8OCyI45Yxsc+dkmFVUmSNLeOXqY7pfTMlNLvdbLN\nheZ1r/td7rprF81nf9x11y5e+9o3VFyZJEmz87aXGfHsD0nSQuYNxTIyNjbWeDb9XUq3bdvW0Xok\nSdoXHqnIyNSzP54MjAHLga8Dnv0hScqboSIjvb29/MZvrObLX/5t4BdNaw7mmc8816EPSVLWHP7I\nTEQQ8UiaJ2oWryVJyptHKjIyMjLCtdd+idaJmiklrr3WiZqSpLx5pCIjUydqjgBXAaM4UVOStBB4\npCIjkxM1nwvc0LRmFeBETUlS3gwVGent7eXII5exY8etwF8CjwV+CryTI49c5tCHJClrhoqMjIyM\nsGPHdoojE29uWrOKHTtucE6FJClrzqnIyOScipta1hSvnVMhScqZoSIjXV1dQDQezYplS5Z4YEmS\nlC9DRUauuuoqih/JoRRzKq4A3t943cWmTZsqrE6SpNn51TcjW7duBfYAJ9A6pwJuaKyXJClPhoqM\nHH300RRHKm5j6tkf7wa6GuslScqTwx8Zec5zngNMAMdRHKl4GfD7jdcTPO95z6uwOkmSZmeoyMgx\nxxzD5JGKjS3/7WLZsmUVVidJ0uwMFRkpzv6YAD5Mcevz/wROBz4ETHj2hyQpa35KZWRiYqLx7BPA\nS5vWPBOA3bt3d7okSZLmzSMVGSnu/dEFXM/U4Y/rgS7v/SFJypqhIjsTwEeYOvzx4cZySZLyZajI\nyORluj8BrATWAr3AZYCX6ZYk5c1QkRGHPyRJC1mpoSIiXhMRN0bEeOPxtYg4v8w2Fz6HPyRJC1PZ\nRyp+BLwF6AP6gWuBz0bESSW3uyA5/CFJWshKDRUppS+klK5OKY2llLallN4G3AOcUWa7C5XDH5Kk\nhaxjcyoioisiXgI8CviPTrW78Dj8IUlamEq/+FVEnEwRIg4B7gZekFL6XtntLkRThz8eevGrbdu2\nsWLFik6XJUnSvHTiiprfA04BeoAXAf8QEWcZLB5q6vBH811K34XDH5Kk3JUeKlJKu4HvN15eHxGn\nA28CXjvTvxkYGKCnp2fKslqtRq1WK63OfEwAx1PcpXSvVcAN1ZQjScpOvV6nXq9PWTY+Pl5RNZOq\nuPdHF3DwbBts2LCBvr6+DpWTj2L4o/kupY8DvgB8HOhy+EOSBEz/RXt4eJj+/v6KKiqUGioi4t3A\nVRSfko8GLgTOBs4rs92FavIupe8GPg1sal7L/fffX0ldkiTNR9lnfzwWuIJiXsVmimtVnJdSurbk\ndhekybuUXglspZhXcQXwfuDRvOtd762qNEmS5lTqkYqU0ivLfP8DTTFRE4prhK2idV7F8PA3GR0d\ndQhEkpQl7/2Rkd7eXvr6TgMCuLll7c1AeFVNSVK2DBWZufjit8y6fsmSKubWSpI0N0NFZu68885Z\n1//whz/sUCWSJO0bQ0Vmtm/f/rDWS5JUFY+lZykobpHy50y9quauKouSJGlWhooseVVNSdLC4/BH\nZpYtW8ZsZ38U6yVJyo9HKjJz3HHHMdvwx/HHH19hdZIkzcxQkZnbbruN2YY/PPtDkpQrhz+yNPPw\nhyRJufJIRWYc/pAkLVSGisw4/CFJWqgMFVnqorhb/F8yeaTi3ThaJUnKmaEiM8XwxwRwHA89UrHT\n4Q9JUrb86puZiYkJZpuouXv37s4XJUnSPBgqMtPVNfuPxLuUSpJy5SdUZoqJmjOf/eFETUlSrgwV\nmSnuQjrz2R/epVSSlCuHPzLjvT8kSQuVoSIzxdkfM/PsD0lSrgwVmSnmVMzMORWSpFwZKjIz15wJ\n51RIknJlqJAkSW1hqJAkSW1RaqiIiIsj4hsR8bOI2B4R/xIRvWW2KUmSqlH2kYqnAx8BngKsBg4C\nvhgRjyy5XUmS1GGlXvwqpbS2+XVEvBy4HegH/r3Mthe2ma+oKUlSrjp9Rc0jgATs7HC7C8zMV9SU\nJClXHZuoGREBfBD495TSdzvV7kLjFTUlSQtVJ49UXAr8GvC0uTYcGBigp6dnyrJarUatViuptHx4\nRU1J0lzq9Tr1en3KsvHx8YqqmdSRUBERlwBrgaenlP5nru03bNhAX19f+YVlyCtqSpLmMt0X7eHh\nYfr7+yuqqFB6qGgEiucBZ6eUZv/ElFfUlCQtWKWGioi4FKgBzwXujYi9EwLGU0o/L7NtSZLUWWVP\n1HwNcDjwb8CPmx4XlNzuAWii6gIkSZpV2dep8DLg+yWAQ4CTmDyNdBfQzf33319ZVZIkzcYP/SxN\nAMfy0NNKl/DpT/9jBfVIkjQ3Q0VmJq9D8Z1p19966xijo6OdK0iSpHkyVGTm7LPPnnObLVu2dKAS\nSZL2jaEiM729vZxwwhMorma+p2XtHiB5WqkkKUuGigzVai+h+NF0t6zpxh+ZJClXfkJl6N5776U4\nUjGdxB133NHJciRJmhdDRYZ27Ngx6/qdO73JqyQpP4aKDB155JGzrn/MYx7ToUokSZo/Q0WGjjrq\nqIe1XpKkKhgqsjXz2R+SJOXIUJEtz/6QJC0sfkJlqDi7w7M/JEkLi6EiQ579IUlaiAwVGSrO/ph5\nToVnf0iScmSoyFBxdsfMcyo8+0OSlCNDRYacUyFJWogMFRkq5lTMPPzhnApJUo4MFRkq5lTMPPzh\nnApJUo4MFdmaYPojFRMV1CJJ0twMFRkqhj9mPlLh8IckKUeGigx1dXUx25GKiOh8UZIkzcFQkaGJ\niQmmP1IRQBcpef8PSVJ+DBUZKo5UpMaj+Ue0C4AHHniggqokSZpdqaEiIp4eEZ+LiP+OiImIeG6Z\n7R0oiiMVwUNDBUAX1167pfNFSZI0h7KPVBwK3AC8Du/ZPW/FKaV751Q8dLLm7bdvZ3R0tPOFSZI0\ni1JDRUrp6pTSn6SUPkvx1Vvz8MQnPrHxbObJmldeeWVni5IkaQ7OqcjQ2Wef3Xg2Uw4LRkZGOlWO\nJEnzYqjIUG9vL8uWLZt1G08rlSTlxlCRqb6+vlnXj4+Pd6gSSZLmZ0nVBUxnYGCAnp6eKctqtRq1\nWq2iijpvrtNG77nnng5VIknKTb1ep16vT1mWw5fNLEPFhg0b5vymLknSYjXdF+3h4WH6+/srqqhQ\naqiIiEOB5UzOOHxCRJwC7Ewp/ajMtiVJUmeVfaTiNODLTF4e8q8ay68AXlFy25IkqYNKDRUppS04\nGVSSpEXBD3xJktQWhoqs7R01mmuZJEnVM1RkLXjoVTXTNMskSaqeoSJT9913H5OhovmmYhNAFzt3\n7qykLkmSZmKoyFQRKgB2T7t+ePjbnStGkqR5MFRk6qijjqI4KjHT/IndfOlLX+pgRZIkzc5Qkaln\nPOMZc25z6aWXll+IJEnzZKjI1AUXXDDnNrfccksHKpEkaX4MFZnq7e3lEY84eNZt5rrpmCRJnWSo\nyNgv//LRVZcgSdK8GSoy1t3dPfdGkiRlwlCRsZmHNybmWC9JUucZKjJ2//33U1z8KjH1AljF67vv\nvruSuiRJmo6hImN79uyh+BFNTLv+rrvu6mg9kiTNxlCRsYMPPhjYM+s2XgBLkpQLQ0XGTj755Dm3\nefvb315+IZIkzYOhImNvfvObZ1lbDIncdNNNnSlGkqQ5GCoytmbNmqZXE0w3WXPyxmOSJFXLUJG5\n4loV3cx0Y7FiMqckSdUzVGQupcTMkzWLoOFkTUlSDgwVmXvkIx/Z9Cox9Uc2AXTxqle9qrNFSZI0\nDUNF5l70ohc1nnVThIho2WKCH/zgBx2tSZKk6RgqMnf55Zc3nu1p+u9D7wny8Y9/vEMVSZrJyMgI\nV111FaOjo1WXkp3BwUHe8Y53OFx7gFtSdQHaF90UoWJP0/PCq1/9ag477DDWrVtXUW1S+d71rndx\nzTXXcN555/HWt7616nIetHPnTtatW8/g4KYHl61Zs5Z6fSNLly6tsLKpBgcH+frXv86ZZ57Jueee\n25E2x8bGeMpTnsaOHdsfXHbooY/mpptu5IQTTuhIDeqglFKpD+D1wK3A/cBW4MmzbNsHpKGhoaRJ\nq1atShQTKhJ0Nz3vajzw4WMRPLorbn+ux5IMasix/6LxKL/9yy67rOo/15UaGhra2xd9qeTP9pke\npQ5/RMSLgb8C/hQ4FbgRGIyIo8ps90Bz/fXXN73ae5Ri7xyL6e8LIh14cj99enfVBcyhqv7rovic\nK6v9yeHg3/qt3yLiIG644YaS2tJcyh7+GAD+JqX0DwAR8Rrg2cArgPeV3PYB5dBDD+Xee+9tvNr7\nP+dD51ZIB57p5xHlI/f6irPEqtE8XFuWw4CPAmcB1wGv5/TTn8oDD3hhwCqUFioi4iCgH3j33mUp\npRQRm4Ezy2r3QHXPPfcQ0XrmR+7f3KR2yf133fqqa/+jwIWN5xcCiV271nP55Zfz8pe/vMR2NZ0y\n4+tRFPF0e8vy7cDRJbZ7wLrooouqLkGSMnNWy+uzAbjmmms6X4ryPPtjYGCAnp6eKctqtRq1Wq2i\nivJwxRVX8M1vfpObbx6h+m8ekpSD65g8UgGwBYBzzjmnkmo6pV6vU6/XpywbHx+vqJpJkYqzLtr/\nxsXwx33AC1NKn2tafjnQk1J6wTT/pg8YGhoaoq+vr5S6DgQrV65kZGSEvMdxJQnKn9NxGHAJxRGK\nLcAbOOigBxblnIrh4WH6+/sB+lNKw1XUUNqRipTSrogYAs4BPgcQxaSAc4APl9XuYnDLLbcATDPH\nQpJyVOaR1XuA9U2vl/CNb3yzxPY0m7KnBH8A+J2IuCgifhX4a+BRwOUlt7so7D0vWJIWr8nActll\nl5HSLlatWlVhPYtbqXMqUkpXNq5J8Q5gGXADsCal9NMy211sDBaSpByUPlEzpXQpcGnZ7UiSpGp5\nQzFJktQWhgpJktQWhgpJktQWhgpJktQWhgpJktQWhgpJktQWhgpJktQWhgpJktQWhgpJktQWhgpJ\nktQWhgpJktQWhgpJktQWhgpJktQWhgpJktQWhgpJktQWhgpJktQWhgpJktQWhgpJktQWhgpJktQW\nhgpJktQWhgpJktQWhgpJktQWhopM1ev1qkvIhn1RsB8m2RcF+2GSfZGH0kJFRPxhRHw1Iu6NiJ1l\ntXOg8n+QSfZFwX6YZF8U7IdJ9kUeyjxScRBwJfCxEtuQJEmZWFLWG6eU/gwgIl5WVhuSJCkfzqmQ\nJEltUdqRiv10CMDNN99cdR2VGx8fZ3h4uOoysmBfFOyHSfZFwX6YZF9M+ew8pKoaIqU0/40j3gO8\nZZZNEnBSSmmk6d+8DNiQUnrMPN5/HfCpeRckSZJaXZhS+nQVDe/rkYr3A5fNsc3397MWgEHgQuAH\nwM8fxvtIkrTYHAI8nuKztBL7FCpSSjuAHSXVsvf9K0lXkiQdAL5WZeOlzamIiGOBxwDHA90RcUpj\n1baU0r1ltStJkqqxT3Mq9umNIy4DLppm1W+klK4rpVFJklSZ0kKFJElaXLxOhSRJagtDhSRJaovS\nQ0VEHB8RfxcR34+I+yJiNCLeHhEHtWx3bER8oXEDsp9ExPsioqtlm1+PiOsi4v6I+GFEvHma9p4R\nEUMR8fOIGFmIlwmPiNdHxK2N/dwaEU+uuqb9FREXR8Q3IuJnEbE9Iv4lInqn2e4dEfHjxu/IlyJi\necv6gyPioxFxR0TcHRH/HBGPbdlmaUR8KiLGI+LOxu/doWXv4/6IiLdGxEREfKBl+aLoh4g4JiI+\n2diP+yLixojoa9nmgO6LiOiKiHc2/W3cFhFvm2a7A64fIuLpEfG5iPjvxv8Hz51mm47sd8zjs6cs\ns/VDRCyJiL+IiG9HxD2Nba6IiF9ueY+8+iGlVOoDWAP8PXAOxfmzvwn8BHhf0zZdwE0U59Y+qfFv\nbgf+vGmbRwP/A1wBnARcANwLvLJpm8cD9wDvA1YCrwd2AeeWvZ9t7K8XU1yj4yLgV4G/AXYCR1Vd\n237uzyZgfeNn9iTg/1Fch+SRTdu8pbGPvwmcDPwrMAY8ommbjzX+3dnAqRSnTX2lpa2rgGHgNOCp\nwAiwseo+mKZPnkxxPZfrgQ8stn4AjgBuBf4O6Kc4Q2w1cMJi6gvgDxt/584HjgP+N/Az4A0Hej80\n9vkdwPOAPcBzW9Z3ZL+Zx2dPVf0AHN6o64XACuB0YCvwjZb3yKofqvqF+n2KU0v3vn4WxYf/UU3L\nXg3cCSxpvH4tcMfe141l7wG+2/T6L4Bvt7RVBzZV9T/PfvTNVuBDTa8D+C/gD6qurU37dxQwAfyv\npmU/BgaaXh8O3A9c0PT6F8ALmrZZ2Xif0xuvT2q8PrVpmzXAbuDoqve7qabDgFuAZwJfZmqoWBT9\nALwX2DLHNgd8XwCfB/62Zdk/A/+wyPphgoeGio7sN/P47KmyH6bZ5jSK8PG4XPuhqjkVR1Ck0L3O\nAG5KKd3RtGwQ6AGe2LTNdSml3S3brIyInqZtNre0NQic2a7CyxTFkFA/cM3eZan46W5mgezDPBxB\ncTn3nQCNZojfAAAE9ElEQVQRcQJwNFP3+WfA15nc59MorqnSvM0twG1N25wB3JlSur6prc2Ntp5S\nxo7sp48Cn08pXdu8cJH1w3OAb0XElVEMiQ1HxCv3rlxEffE14JyIWAEQxbV8nkZxdG8x9cMUHd7v\n+Xz25GTv38+7Gq/7yawfOh4qGuNibwD+umnx0cD2lk23N617uNscHhEH72/NHXQU0M30+3D0Qzdf\nWCIigA8C/55S+m5j8dEUv9yz7fMy4IHGH5aZtjma4nDdg1JKeyjCSxZ9FxEvAVYBF0+zetH0A/AE\niiOPtwDnURy+/XBErG+sXyx98V7gH4HvRcQDwBDwwZTSZxrrF0s/tOrkfs/ncyULjc+w9wKfTind\n01h8NJn1w35fUTP27+Ziv0IxtvOPKaVP7G/braW06X1UvkuBX6P4NraoRMTjKALV6pTSrqrrqVgX\nxbjwHzde3xgRJwOvAT5ZXVkd92JgHfAS4LsUgfNDEfHjlNJi6gfNISKWAP9E8bn6uorLmdXDOVLx\nfoqJhDM9TqLp5mIRcQxwLcW31Fe3vNdPKJJns2VN62bbJs1jm5+llH4x3x2r0B0U42XT7cNPHrr5\nwhERlwBrgWeklP6nadVPKILhbPv8E+AREXH4HNu0znjuprhUfA591w/8EjAcEbsiYhfFxKo3Nb6l\nbmdx9AMUE65vbll2M8VkRVg8vxPvA96bUvqnlNJ3UkqfAjYweSRrsfRDq07u93w+eyrVFCiOBc5r\nOkoBGfbDfoeKlNKOlNLIHI/djR34FYpJad8EXjHN2/0H8KSIOKpp2XnAOEWC37vNWY3OaN7mlpTS\neNM257S893mN5dlrfIMdomkfGkMG51DxTWIejkageB7FJdpva16XUrqV4pe2eZ8Ppxjr27vPQxST\nipq3WUnxIbT3Z/sfwBERcWrT259D8cfp6+3cn/20mWJW9SrglMbjW8BG4JSU0vdZHP0A8FWKyWTN\nVgI/hEX1O/Eoii8RzSZo/F1eRP0wRYf3ez6fPZVpChRPAM5JKd3Zskl+/dCBGa3HAKPAFxvPl+19\nNG3TBdxIMTTy6xQzU7cD72za5nCKGcFXUBxCfzHF6aO/3bTN44G7Kc4CWUlxmOgBikPOHZvF+zD7\n6wLgPqaeUroD+KWqa9vP/bmUYgbx05t/9sAhTdv8QWMfn0Pxwfuvjd+ZR7S8z63AMyi+9X+Vh542\ntYnig/rJFEMstwCfrLoPZumb1rM/FkU/UEyy+wXFN/ITKYYA7gZespj6AriMYkLdWorTal9AMfb9\n7gO9H4BDKYL1Koog9X8ar4/t5H4zj8+eqvqBYnrCZynC9pOY+vfzoFz7oROd9jKKNN78mAD2tGx3\nLMU1DO5p7MxfAF0t25wMbKH40L0N+P1p2juLIr3d3/glXF/V/zgPo89eR3He8f0UCfK0qmt6GPsy\nMc3Pfw9wUct2b6cIjfdRzDpe3rL+YOAjFENEd1Ok98e2bHMExTf/cYog87fAo6rug1n65lqaQsVi\n6geKD9JvN/bzO8ArptnmgO4Lig+UD1B8INzb+Hv1Z7Scwncg9gPF0N90fxs+0en9Zh6fPVX0A0XQ\nbF239/VZufaDNxSTJElt4b0/JElSWxgqJElSWxgqJElSWxgqJElSWxgqJElSWxgqJElSWxgqJElS\nWxgqJElSWxgqJElSWxgqJElSWxgqJElSW/x/9vbIq9gzcd0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b75d850>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create graph of how the MSE converges\n",
    "x_values = np.arange(0, 10000, 1).tolist()\n",
    "\n",
    "plt.scatter(x_values, train_errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Compare your results above to LinearRegression in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121, 1)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at the parameters from tensor flow\n",
    "tensor_flow_params = sess.run(W)\n",
    "tensor_flow_params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sk learn linear regression\n",
    "\n",
    "#instantiate model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "#fit to the training data\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "#get parameters\n",
    "lin_regression_coefs = linear_model.coef_.reshape(121,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compare the parameters between the results from the neural net and the linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3: In Week 5, we found that the best ridge regularization parameter for this data was alpha=11.8. Try to add the same amount of regularization to the tensorflow model above, then compare with ridge regression in scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer Perceptron (MLP)\n",
    "\n",
    "![](mlp.png)\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Build a multi-layer perceptron to predict crime rates.\n",
    "\n",
    "Start with two hidden units. You should be able to define one matrix transforms the inputs to the hidden layer, and a second matrix that will transform the hidden layer to the output.\n",
    "\n",
    "Don't forget add bias at each step and to apply a nonlinear transformation to the hidden layer (e.g. tf.nn.sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim_hidden = 2\n",
    "\n",
    "# input\n",
    "\n",
    "# output\n",
    "\n",
    "# Input to hidden\n",
    "\n",
    "\n",
    "# Hidden to output\n",
    "\n",
    "\n",
    "# Model\n",
    "\n",
    "\n",
    "# Loss\n",
    "\n",
    "\n",
    "# Optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have something working, it is time to tune your network to find the right number of hidden layers and amount of regularization.\n",
    "\n",
    "1. Use your code block from above that performs gradient descent steps and records intermediate results.\n",
    "2. You might want to force the optimizer to be stochastic. That is, feed it 100 random training examples at each step instead of the whole training dataset.\n",
    "3. Start with two hidden units and try to get the regularization right. Then slowly increase the number of hidden units and continue tuning the regularization.\n",
    "4. If the training error is high, you have too much bias. If the training and testing errors are very different, you have too much variance. If the training or testing errors are jumping all over the place, your step size is too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Add _another_ hidden layer.\n",
    "\n",
    "Can you decrease the MSE on the test set even further?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dim_h1 = 8\n",
    "dim_h2 = 8\n",
    "\n",
    "# input\n",
    "x = tf.placeholder(tf.float32, [None, dim_input])\n",
    "\n",
    "# target\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# Input to hidden 1\n",
    "W1 = tf.Variable(tf.random_normal([dim_input, dim_h1]))\n",
    "b1 = tf.Variable(tf.random_normal([dim_h1]))\n",
    "\n",
    "# Hidden 1 to hidden 2\n",
    "W2 = tf.Variable(tf.random_normal([dim_h1, dim_h2]))\n",
    "b2 = tf.Variable(tf.random_normal([dim_h2]))\n",
    "\n",
    "# Hidden 2 to output\n",
    "W3 = tf.Variable(tf.random_normal([dim_h2, dim_output]))\n",
    "b3 = tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "# Model\n",
    "H1 = tf.nn.tanh(tf.matmul(x, W1) + b1)\n",
    "H2 = tf.nn.tanh(tf.matmul(H1, W2) + b2)\n",
    "y = tf.matmul(H2, W3) + b3\n",
    "\n",
    "# Loss\n",
    "mse = tf.reduce_mean(tf.square(y - y_))\n",
    "lam = .4\n",
    "reg = tf.reduce_mean(lam * tf.square(W1)) + \\\n",
    "    tf.reduce_mean(lam * tf.square(W2)) + \\\n",
    "    tf.reduce_mean(lam * tf.square(W3))\n",
    "loss = mse + reg\n",
    "\n",
    "# Optimizer\n",
    "train_step = tf.train.AdamOptimizer(0.0005).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    idx = np.random.choice(X_train.shape[0], 150, replace=True)\n",
    "    X_batch = X_train.iloc[idx, :]\n",
    "    y_batch = y_train.iloc[idx, :]\n",
    "    if i % 1000 == 0:\n",
    "        train_mse = sess.run(mse, {x: X_batch, y_: y_batch})\n",
    "        test_mse = sess.run(mse, {x: X_test, y_: y_test})\n",
    "        print 'Iteration: {:04} \\t Train Loss: {:.3} \\t Test Loss: {:.3}'.format(i, train_mse, test_mse)\n",
    "    batch_update(sess, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Machine_learning_python2]",
   "language": "python",
   "name": "conda-env-Machine_learning_python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
