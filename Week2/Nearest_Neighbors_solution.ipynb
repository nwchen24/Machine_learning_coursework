{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and test a Nearest Neighbors classifier.\n",
    "\n",
    "- We will try to build the best possible nearest neighbors classifier for a specific data set. By 'best', we mean _highest accuracy_\n",
    "- Use a train/dev/test split\n",
    "- Experiment with as many hyper-parameters as possible\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Iris data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Iris data to use for experiments. The data include 50 observations of each of 3 types of irises (150 total). Each observation includes 4 measurements: sepal and petal width and height. The goal is to predict the iris type from these measurements.\n",
    "\n",
    "<http://en.wikipedia.org/wiki/Iris_flower_data_set>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data, which is included in sklearn.\n",
    "iris = load_iris()\n",
    "print 'Iris target names:', iris.target_names\n",
    "print 'Iris feature names:', iris.feature_names\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a quick look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X.shape\n",
    "print X[:5]\n",
    "print y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data is fairly well behaved. For now, we are going to skip EDA and feature engineering---which would normally be essential steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break off a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Shuffle the data, but make sure that the features and accompanying labels stay in sync.\n",
    "test_size = 50  # 1/3 of the data\n",
    "test_idx = np.random.choice(X.shape[0], test_size, replace=False)\n",
    "train_idx = np.setdiff1d(range(X.shape[0]), test_idx)\n",
    "\n",
    "# Split into train and test.\n",
    "X_train, y_train = X[train_idx, :], y[train_idx]\n",
    "X_test, y_test = X[test_idx, :], y[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we set aside the test set. We should only touch the test set once!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the training set into development train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "dev_size = 30\n",
    "devtest_idx = np.random.choice(train_idx, dev_size, replace=False)\n",
    "devtrain_idx = np.setdiff1d(train_idx, devtest_idx)\n",
    "\n",
    "# Split into train and dev.\n",
    "X_devtrain, y_devtrain = X[devtrain_idx, :], y[devtrain_idx]\n",
    "X_devtest, y_devtest = X[devtest_idx, :], y[devtest_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print X.shape\n",
    "print X_test.shape\n",
    "print X_train.shape\n",
    "print X_devtrain.shape\n",
    "print X_devtest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement a distance function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a distance function that returns the distance between 2 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def EuclideanDistance(v1, v2):\n",
    "    sum = 0.0\n",
    "    for index in range(len(v1)):\n",
    "        sum += (v1[index] - v2[index]) ** 2\n",
    "    return sum ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for fun, let's compute all the pairwise distances in the training data and plot a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dists = []\n",
    "for i in range(len(X_train) - 1):\n",
    "    for j in range(i + 1, len(X_train)):\n",
    "        dist = EuclideanDistance(X_train[i], X_train[j])\n",
    "        dists.append(dist)\n",
    "        \n",
    "fig = plt.hist(dists, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nicer Versions of the above two cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python is great for data science because there are a ton of high-level, commonly used, and often vectorized operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use vectorized numpy functions\n",
    "def EucliceanDistance(v1, v2):\n",
    "    dist = np.sum((v1 - v2) ** 2) ** 0.5\n",
    "    return dist\n",
    "\n",
    "# There should be a function for this\n",
    "from scipy.spatial.distance import pdist\n",
    "dists = pdist(X_train, 'euclidean')\n",
    "\n",
    "fig = plt.hist(dists, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-NN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now let's create a class that implements a Nearest Neighbors classifier. We'll model it after the sklearn classifier implementations, with fit() and predict() methods.\n",
    "\n",
    "<http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "class NearestNeighbors:\n",
    "    \n",
    "    def __init__(self, metric=EuclideanDistance):\n",
    "        self.metric = metric\n",
    "        \n",
    "    def fit(self, data, labels):\n",
    "        self.train_data = data\n",
    "        self.train_labels = labels\n",
    "        \n",
    "    def predict(self, test_data):\n",
    "        results = []\n",
    "        for item in test_data:\n",
    "            results.append(self._predict_item(item))\n",
    "        return results\n",
    "    \n",
    "    def _predict_item(self, item):\n",
    "        \"\"\"\n",
    "        Return the label of the training example closest to item\n",
    "        \"\"\"\n",
    "        distances = cdist([item], self.train_data, self.metric)\n",
    "        best_label = self.train_labels[np.argmin(distances)]\n",
    "        return best_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test\n",
    "\n",
    "See how well the classifier performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 30  correct: 29  accuracy: 0.967\n"
     ]
    }
   ],
   "source": [
    "clf = NearestNeighbors()\n",
    "clf.fit(X_devtrain, y_devtrain)\n",
    "preds = clf.predict(X_devtest)\n",
    "\n",
    "correct = np.sum(preds==y_devtest)\n",
    "total = len(y_devtest)\n",
    "print 'total: {}  correct: {}  accuracy: {:.3}'.format(total, correct, 1.0 * correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k nearest neighbors\n",
    "\n",
    "The implementation above only allows for a single nearest neighbor; that is, the classifier predicts the label of the closest available point. What about using more than one nearest neighbor. Typically, this means to make a prediciton we:\n",
    "\n",
    "1. Find the k closest points (according to our distance metric) to the query point.\n",
    "2. Find the majority label of those k points found in (1)\n",
    "3. Return the label in (2) as the prediction\n",
    "\n",
    "Try implementing this strategy below.\n",
    "Hint: Check out the `most_common` method in `Counter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class OurKNearestNeighbors:\n",
    "    # Initialize an instance of the class.\n",
    "    def __init__(self, k, metric=EuclideanDistance):\n",
    "        self.metric = metric\n",
    "        self.k = k\n",
    "    \n",
    "    # No training for Nearest Neighbors. Just store the data.\n",
    "    def fit(self, train_data, train_labels):\n",
    "        self.train_data = train_data\n",
    "        self.train_labels = train_labels\n",
    "    \n",
    "    # Make predictions for each test example and return results.\n",
    "    def predict(self, test_data):\n",
    "        results = [self._predict_item(item) for item in test_data]\n",
    "        return results\n",
    "    \n",
    "    # Private function for making a single prediction using KNN.\n",
    "    def _predict_item(self, item):\n",
    "        dists = cdist([item], self.train_data, self.metric).flatten()\n",
    "        k_closest = self.train_labels[np.argsort(dists)][:self.k]\n",
    "        best_label = Counter(k_closest).most_common(1)[0][0]\n",
    "        return best_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test\n",
    "\n",
    "`NearestNeighbors` and `OurKNearestNeighbors` should give the same prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfk = OurKNearestNeighbors(k=1)\n",
    "clfk.fit(X_devtrain, y_devtrain)\n",
    "preds = clfk.predict(X_devtest)\n",
    "\n",
    "# test\n",
    "clf = NearestNeighbors()\n",
    "clf.fit(X_devtrain, y_devtrain)\n",
    "preds_nn = clf.predict(X_devtest)\n",
    "\n",
    "print \"Percentage of same predictions: \", np.mean(preds == preds_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking k: the number of neighbors to use in classification\n",
    "\n",
    "Implement a way to pick the number of neighbors to use in the classifier. We already have a test set, so simply extend the procedure in the previous code cell to run over different numbers of neighbors. Plot the test set performance versus the number of neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## code goes here!\n",
    "\n",
    "for k in range(1, 15):\n",
    "    clfk = OurKNearestNeighbors(k=k)\n",
    "    clfk.fit(X_devtrain, y_devtrain)\n",
    "    preds = clfk.predict(X_devtest)\n",
    "    \n",
    "    print \"k = {} ; accuracy: {:.4}\".format(k, np.mean(preds == y_devtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This isn't really enough information. Let's try using more train/dev splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - Write a function that splits a training dataset randomly, builds a kNN classifier, and reports the acccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "import pdb\n",
    "\n",
    "def split_and_knn_classifier(data, labels, k, test_size=test_size):\n",
    "    \n",
    "    # Set aside the index\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=test_size)\n",
    "    \n",
    "    # Build a KNN classifier and record the accuracy\n",
    "    clfk = OurKNearestNeighbors(k=k, metric='euclidean')\n",
    "    clfk.fit(train_data, train_labels)\n",
    "    preds = clfk.predict(test_data)\n",
    "    accuracy = np.mean(preds == test_labels)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "split_and_knn_classifier(X_train, y_train, k=1, test_size=20)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 - Run the above function 500 times for each value of k between 1 and 15 to get the mean test accuracy for that value of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k in range(1, 20):\n",
    "    mean_accuracy = np.mean([split_and_knn_classifier(X_train, y_train, k, test_size=20) for i in range(1000)])\n",
    "    print \"k = {}, mean_accuracy = {:.3}\".format(k, mean_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 (optional) - If you would like, now would be the time to experiment with different distance metrics. Repeat the above step and see what the results look like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "Use Occam's razor to select the best model.\n",
    "\n",
    "Once you have chosen the hyper-parameters for the model (k and distance metric), it is time to deploy the model. Now is the time to test our model on the test set, so that we know what to expect after deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric = 'euclidean'\n",
    "k = 10\n",
    "finalclf = OurKNearestNeighbors(k=k, metric=metric)\n",
    "finalclf.fit(X_train, y_train)\n",
    "preds = finalclf.predict(X_test)\n",
    "accuracy = np.mean(preds == y_test)\n",
    "\n",
    "print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Before deploying the model, we could actually incorporate the test data into our training set. The only thing that matters now, is how well the model generalizes in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the results\n",
    "\n",
    "We've been a litte haphazard so far, we should have plotted the data and some results to get an idea of how the algorithm is performing. Plot the data with the true labels as colors, and plot it with some fitted labels, for differing values of k, to see how our KNN algorithm is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "\n",
    "# clfk = OurKNearestNeighbors(k=k)\n",
    "# clfk.fit(train_data, train_labels)\n",
    "# preds = clfk.predict(test_data)\n",
    "\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF', '#00FF00'])\n",
    "\n",
    "plt.figure(figsize=(8,8)) \n",
    "p = plt.subplot(2, 2, 1)\n",
    "p.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright)\n",
    "plt.title(\"Iris Test Data: X1 vs X2\")\n",
    "\n",
    "p = plt.subplot(2, 2, 2)\n",
    "p.scatter(X_test[:, 2], X_test[:, 3], c=y_test, cmap=cm_bright)\n",
    "plt.title(\"Iris Test Data: X3 vs X4\")\n",
    "\n",
    "p = plt.subplot(2, 2, 3)\n",
    "p.scatter(X_test[:, 0], X_test[:, 1], c=preds, cmap=cm_bright)\n",
    "plt.title(\"Iris Test Data: X1 vs X2 [pred colors]\")\n",
    "\n",
    "p = plt.subplot(2, 2, 4)\n",
    "p.scatter(X_test[:, 2], X_test[:, 3], c=preds, cmap=cm_bright)\n",
    "plt.title(\"Iris Test Data: X3 vs X4 [pred colors]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Implement variable scaling. \n",
    "\n",
    "That is, scale each feature to its normalized z-score. Make sure not to contaminate the test set..\n",
    "\n",
    "Repeat steps one and two above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def split_and_knn_classifier(data, labels, k, test_size=test_size):\n",
    "    \n",
    "    # Set aside the index\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=test_size)\n",
    "    \n",
    "    # Build a KNN classifier and record the accuracy\n",
    "    ss = StandardScaler()\n",
    "    clfk = OurKNearestNeighbors(k=k, metric='euclidean')\n",
    "    pipeline = Pipeline([('scale', ss), ('knn', clfk)])\n",
    "    pipeline.fit(train_data, train_labels)\n",
    "    preds = pipeline.predict(test_data)\n",
    "    accuracy = np.mean(preds == test_labels)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "split_and_knn_classifier(X_train, y_train, k=1, test_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k in range(1, 20):\n",
    "    mean_accuracy = np.mean([split_and_knn_classifier(X_train, y_train, k, test_size=20) for i in range(1000)])\n",
    "    print \"k = {}, mean_accuracy = {:.3}\".format(k, mean_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: Did variable scaling improve anything? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: What are some other things you could try? (In the context of K nearest neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
